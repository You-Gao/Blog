---
date:             2025-10-08
description:  >-
    stopping my use
title: >-
    relinquishing ai
layout:           style
name: >-
    relinquishing ai
comment: 
---

# relinquishing ai

I am giving it up. I have run the experiment of using Gen AI, and my conclusion is that it is a threat to my own creativity and critical thinking.

The feeding of real-time information through "*RAG*" still doesn't help disinformation. So I am just going back to using Google to get information about topics. Although the one actual benefit, which doesn't outweigh the cons, is that it does well in matching "*concepts*" of topics.

## llms can't do novel design

The killing of creativity is due to the fact that it can't do new things that people haven't done. It's an age-old critique that has been said about them.

Yes, they can design, if you consider design to be selecting from previous patterns of design, whether that is in product, software, or systems design. The point is llms can't imagine new designs, it can't do the work of researching, it can't do the work of theorizing, and surely not making it a real thing.

These companies are soon to run out of "*good code*" to even train their models on. Even worse, it is an understood phenomenon of any statistical model that when you feed in synthetic data from its output, your model will "*necessarily*" become degenerate[^1]. In the case of an image generation model, it degrades to noise, and in the case of LLMs, it might word-for-word give you the whole text document.

## llms have no understanding of grammar[^2]

There's something more important than the simple understanding of sentences or words. It's knowing how to interpret those sentences, how to form more sentences, and subsequently understanding the "*semantic content*" of them.

In computers, there is the ISA rule set, in linguistics, we have parts of speech, and in formal logic, there are rules of inference. The grammar difference accounts for the fundamental difference between humans and LLMs, even more so than the "*stochastic parrot*" bullet. 

LLMs aren't like proof programs such as Coq and exploit word information but not semantic information.

<br/>
---
[^1]: [https://en.wikipedia.org/wiki/Model_collapse](https://en.wikipedia.org/wiki/Model_collapse)
[^2]: [https://en.wikipedia.org/wiki/Universal_grammar](https://en.wikipedia.org/wiki/Universal_grammar)

